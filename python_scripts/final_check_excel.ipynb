{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82130643-1b66-4dda-bd12-e0706ca4d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define path\n",
    "excel_path = '/Users/emilsafarov/Library/CloudStorage/OneDrive-Personal/CF/A6/1_sourceing_open_data/US project/Data/Output data/untitled folder/essential.xlsx'\n",
    "\n",
    "# Step 2: Load all sheets into a dictionary\n",
    "all_sheets = pd.read_excel(excel_path, sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e295901d-0b02-4855-a03d-0850153f65d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sheets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f50646-a0d8-4ab7-8eba-387283529c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet: Population â†’ 3144 rows\n",
      "Sheet: CO-EST2024-POP â†’ 3144 rows\n",
      "Sheet: air_pollution â†’ 3198 rows\n",
      "Sheet: children_lunch â†’ 3198 rows\n",
      "Sheet: children_poverty â†’ 3198 rows\n",
      "Sheet: dentists_raw_value â†’ 3198 rows\n",
      "Sheet: disconnected_youth â†’ 3198 rows\n",
      "Sheet: drinking_water â†’ 3199 rows\n",
      "Sheet: drive_alone â†’ 3198 rows\n",
      "Sheet: drug_overdose_deaths â†’ 3198 rows\n",
      "Sheet: exercise_access â†’ 3198 rows\n",
      "Sheet: firearm_fatalities â†’ 3198 rows\n",
      "Sheet: flu_vaccinations_raw_value â†’ 3198 rows\n",
      "Sheet: food_environment_index â†’ 3198 rows\n",
      "Sheet: food_insecurity â†’ 3198 rows\n",
      "Sheet: frequent_physical_distress â†’ 3198 rows\n",
      "Sheet: high_housing_cost â†’ 3198 rows\n",
      "Sheet: high_school_graduation â†’ 3198 rows\n",
      "Sheet: hiv_prevalence_raw_value â†’ 3198 rows\n",
      "Sheet: homeownership â†’ 3198 rows\n",
      "Sheet: homicides â†’ 3198 rows\n",
      "Sheet: housing_facilities â†’ 3198 rows\n",
      "Sheet: income_inequality â†’ 3198 rows\n",
      "Sheet: injury_deaths â†’ 3198 rows\n",
      "Sheet: insufficient_sleep_raw_value â†’ 3198 rows\n",
      "Sheet: juvenile_arrests â†’ 3198 rows\n",
      "Sheet: life_expectancy â†’ 3198 rows\n",
      "Sheet: limited_access_healthy_foods â†’ 3198 rows\n",
      "Sheet: long_commute â†’ 3198 rows\n",
      "Sheet: mammography_screening â†’ 3198 rows\n",
      "Sheet: math_scores â†’ 3198 rows\n",
      "Sheet: median_income â†’ 3198 rows\n",
      "Sheet: mental_health_providers_raw_val â†’ 3198 rows\n",
      "Sheet: motor_vehicle_crash_deaths â†’ 3198 rows\n",
      "Sheet: not_proficient_english â†’ 3198 rows\n",
      "Sheet: poor_or_fair_health â†’ 3198 rows\n",
      "Sheet: poor_physical_health â†’ 3198 rows\n",
      "Sheet: population_raw_value â†’ 3198 rows\n",
      "Sheet: primary_care_physicians â†’ 3198 rows\n",
      "Sheet: reading_scores â†’ 3198 rows\n",
      "Sheet: severe_housing â†’ 3198 rows\n",
      "Sheet: severe_housing_problems â†’ 3198 rows\n",
      "Sheet: some_college â†’ 3198 rows\n",
      "Sheet: suicides_raw_value â†’ 3198 rows\n",
      "Sheet: traffic_volume â†’ 3198 rows\n",
      "Sheet: unemployment â†’ 3198 rows\n",
      "Sheet: uninsured_raw_value â†’ 3198 rows\n",
      "Sheet: violent_crime â†’ 3198 rows\n",
      "Sheet: fmr â†’ 3251 rows\n"
     ]
    }
   ],
   "source": [
    "for sheet_name, df in all_sheets.items():\n",
    "    print(f\"Sheet: {sheet_name} â†’ {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e8d017-6bb5-4b5b-a55b-6ac5a1e800d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_sheets_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ðŸ” Loop through target sheets and check for duplicate FIPS codes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sheet \u001b[38;5;129;01min\u001b[39;00m target_sheets_1:\n\u001b[1;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m all_sheets[sheet]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIPS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Standardize FIPS to 5-digit strings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_sheets_1' is not defined"
     ]
    }
   ],
   "source": [
    "# ðŸ” Loop through target sheets and check for duplicate FIPS codes\n",
    "for sheet in target_sheets_1:\n",
    "    df = all_sheets[sheet].copy()\n",
    "    \n",
    "    if 'FIPS' in df.columns:\n",
    "        # Standardize FIPS to 5-digit strings\n",
    "        df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n",
    "        \n",
    "        # Count duplicates\n",
    "        duplicates = df['FIPS'].duplicated().sum()\n",
    "        \n",
    "        # Report if any found\n",
    "        if duplicates > 0:\n",
    "            print(f\"â— Sheet '{sheet}' has {duplicates} duplicate FIPS\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Sheet '{sheet}' has no 'FIPS' column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0247687-83bf-4d9c-a94d-5de8464f1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sheets_1 = [\n",
    "    \"air_pollution\", \"children_lunch\", \"children_poverty\", \"dentists_raw_value\",\n",
    "    \"disconnected_youth\", \"drinking_water\", \"drive_alone\", \"drug_overdose_deaths\",\n",
    "    \"exercise_access\", \"firearm_fatalities\", \"flu_vaccinations_raw_value\", \"food_environment_index\",\n",
    "    \"food_insecurity\", \"frequent_physical_distress\", \"high_housing_cost\", \"high_school_graduation\",\n",
    "    \"hiv_prevalence_raw_value\", \"homeownership\", \"homicides\", \"housing_facilities\",\n",
    "    \"income_inequality\", \"injury_deaths\", \"insufficient_sleep_raw_value\", \"juvenile_arrests\",\n",
    "    \"life_expectancy\", \"limited_access_healthy_foods\", \"long_commute\", \"mammography_screening\",\n",
    "    \"math_scores\", \"median_income\", \"mental_health_providers_raw_val\", \"motor_vehicle_crash_deaths\",\n",
    "    \"not_proficient_english\", \"poor_or_fair_health\", \"poor_physical_health\", \"population_raw_value\",\n",
    "    \"primary_care_physicians\", \"reading_scores\", \"severe_housing\", \"severe_housing_problems\",\n",
    "    \"some_college\", \"suicides_raw_value\", \"traffic_volume\", \"unemployment\",\n",
    "    \"uninsured_raw_value\", \"violent_crime\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ea0f6-b46c-4981-a91a-ea2f7e360ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize merged DataFrame\n",
    "merged_df_1 = None\n",
    "\n",
    "# Merge all sheets side-by-side on FIPS\n",
    "for sheet_name in target_sheets_1:\n",
    "    df = all_sheets[sheet_name].copy()\n",
    "\n",
    "    # Standardize FIPS\n",
    "    df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n",
    "\n",
    "    # Keep only one row per FIPS, State, County (take mean of numeric values)\n",
    "    group_keys = ['FIPS']\n",
    "    if 'State' in df.columns and 'County' in df.columns:\n",
    "        group_keys = ['FIPS', 'State', 'County']\n",
    "\n",
    "    df = df.groupby(group_keys, as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    # Rename value columns (but keep FIPS, State, County intact)\n",
    "    df = df.rename(columns={col: f\"{sheet_name}__{col}\" for col in df.columns if col not in group_keys})\n",
    "\n",
    "    # Merge\n",
    "    if merged_df_1 is None:\n",
    "        merged_df_1 = df\n",
    "    else:\n",
    "        merged_df_1 = pd.merge(merged_df_1, df, on=group_keys, how=\"outer\")\n",
    "\n",
    "# âœ… Final merged_df_1 contains FIPS, State, County, and all indicators\n",
    "print(\"âœ… Final merged shape:\", merged_df_1.shape)\n",
    "print(merged_df_1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04972677-8d52-44cd-9189-25738adcf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc3055-8d54-452d-894e-88ad80abd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two population-related sheets\n",
    "pop_df_1 = all_sheets[\"Population\"].copy()\n",
    "pop_df_2 = all_sheets[\"CO-EST2024-POP\"].copy()\n",
    "\n",
    "# Standardize column names\n",
    "pop_df_1.columns = pop_df_1.columns.str.strip()\n",
    "pop_df_2.columns = pop_df_2.columns.str.strip()\n",
    "\n",
    "# Merge on State and County\n",
    "merged_df_2 = pd.merge(pop_df_1, pop_df_2, on=[\"State\", \"County\"], how=\"outer\", suffixes=(\"_pop\", \"_est2024\"))\n",
    "\n",
    "# âœ… Preview result\n",
    "print(\"âœ… Merged shape:\", merged_df_2.shape)\n",
    "print(merged_df_2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad06bfa-b204-4c97-88a7-8adbb44c2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f111847-5598-4ea0-bf89-e7ab5d2ddbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique FIPSâ€“Stateâ€“County mapping\n",
    "fips_lookup_df = merged_df_1[['FIPS', 'State', 'County']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afad5d0-4c6c-46ff-829c-edef0e484cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS to merged_df_2 using State + County as the key\n",
    "merged_df_2 = pd.merge(merged_df_2, fips_lookup_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "\n",
    "# Confirm FIPS added\n",
    "print(\"âœ… FIPS added to merged_df_2\")\n",
    "print(merged_df_2[['FIPS', 'State', 'County']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6fbbd-14fb-410d-9bce-4b312b844946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Filter rows where FIPS was not found (NaN after merge)\n",
    "missing_fips_rows = merged_df_2[merged_df_2['FIPS'].isna()]\n",
    "\n",
    "# ðŸ“Š Show how many are missing\n",
    "print(f\"â— Rows with missing FIPS: {missing_fips_rows.shape[0]}\")\n",
    "\n",
    "# ðŸ‘€ Preview them\n",
    "print(missing_fips_rows[['State', 'County']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6efec8-595b-4523-aa0d-6f1d4c0cdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmr_df = all_sheets[\"fmr\"].copy()\n",
    "\n",
    "# Make sure FIPS is standardized\n",
    "fmr_df['FIPS'] = fmr_df['FIPS'].astype(str).str.zfill(5)\n",
    "\n",
    "# (Optional) Check available columns\n",
    "# print(fmr_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d22538-8f74-4683-afea-cc175857979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract State + County from missing rows\n",
    "missing_fips_rows = merged_df_2[merged_df_2['FIPS'].isna()][['State', 'County']].drop_duplicates()\n",
    "\n",
    "# Check if these rows exist in merged_df_1\n",
    "merged_1_check = pd.merge(missing_fips_rows, merged_df_1[['FIPS', 'State', 'County']], on=['State', 'County'], how='left')\n",
    "print(\"ðŸ”Ž FIPS from merged_df_1:\")\n",
    "print(merged_1_check)\n",
    "\n",
    "# Check if they exist in fmr_df\n",
    "fmr_check = pd.merge(missing_fips_rows, fmr_df[['FIPS', 'State', 'County']], on=['State', 'County'], how='left')\n",
    "print(\"\\nðŸ”Ž FIPS from fmr_df:\")\n",
    "print(fmr_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea086b3d-c0b4-4bcc-9daa-c80e8c6ad4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of manually fixed FIPS codes\n",
    "manual_fips_fixes = {\n",
    "    ('AK', 'Anchorage Municipality'): '02020',\n",
    "    ('AK', 'Juneau City and Borough'): '02110',\n",
    "    ('AK', 'Petersburg Borough'): '02195',\n",
    "    ('AK', 'Sitka City and Borough'): '02220',\n",
    "    ('AK', 'Wrangell City and Borough'): '02275',\n",
    "    ('AK', 'Yakutat City and Borough'): '02282',\n",
    "    \n",
    "    ('CT', 'Capitol Planning Region'): '00001',\n",
    "    ('CT', 'Greater Bridgeport Planning Region'): '00002',\n",
    "    ('CT', 'Lower Connecticut River Valley Planning Region'): '00003',\n",
    "    ('CT', 'Naugatuck Valley Planning Region'): '00004',\n",
    "    ('CT', 'Northeastern Connecticut Planning Region'): '00005',\n",
    "    ('CT', 'Northwest Hills Planning Region'): '00006',\n",
    "    ('CT', 'South Central Connecticut Planning Region'): '00007',\n",
    "    ('CT', 'Southeastern Connecticut Planning Region'): '00008',\n",
    "    ('CT', 'Western Connecticut Planning Region'): '00009',\n",
    "\n",
    "    ('IL', 'LaSalle County'): '17099',\n",
    "    ('IN', 'DeKalb County'): '18033',\n",
    "    ('IN', 'LaGrange County'): '18087',\n",
    "    ('IN', 'LaPorte County'): '18091',\n",
    "    ('NM', 'De Baca County'): '35011',\n",
    "    ('NM', 'DoÃ±a Ana County'): '35013',\n",
    "    ('PA', 'McKean County'): '42083'\n",
    "}\n",
    "\n",
    "# Apply manual corrections\n",
    "merged_df_2['FIPS'] = merged_df_2.apply(\n",
    "    lambda row: manual_fips_fixes.get((row['State'], row['County']), row['FIPS']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521073b-5048-4869-8915-5422fd9d2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6f22c-8af3-43d7-822f-141def19c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Filter rows where FIPS, State, or County is missing\n",
    "missing_key_info = merged_df_2[\n",
    "    merged_df_2[['FIPS', 'State', 'County']].isna().any(axis=1)\n",
    "]\n",
    "\n",
    "# ðŸ‘€ Display them\n",
    "print(\"â— Rows with missing FIPS, State, or County:\")\n",
    "print(missing_key_info[['FIPS', 'State', 'County']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe9c89-97a4-4b66-be66-1ca9f98373ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Count how many FIPS in merged_df_1 also exist in merged_df_2\n",
    "# This tells you how many rows will successfully match during the merge\n",
    "print(\"ðŸ” FIPS match in merged_df_2:\", merged_df_1['FIPS'].isin(merged_df_2['FIPS']).sum())\n",
    "\n",
    "# ðŸ” Count how many FIPS in merged_df_1 also exist in fmr_df\n",
    "# Useful to ensure FMR data can be merged without losing rows\n",
    "print(\"ðŸ” FIPS match in fmr_df:\", merged_df_1['FIPS'].isin(fmr_df['FIPS']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b331a-d983-4a3e-af03-1da8887504f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¦ merged_df_1 (Main dataset) rows:\", merged_df_1.shape[0])\n",
    "print(\"ðŸ“¦ merged_df_2 (Population data) rows:\", merged_df_2.shape[0])\n",
    "print(\"ðŸ“¦ fmr_df (Fair Market Rent) rows:\", fmr_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c140220-50e6-4abf-9ce5-8b70330c5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Find duplicated rows based on FIPS, State, and County\n",
    "fmr_duplicates = fmr_df[fmr_df.duplicated(subset=['FIPS', 'State', 'County'], keep=False)]\n",
    "\n",
    "# ðŸ“Š Show how many duplicated entries\n",
    "print(f\"â— Duplicated rows in fmr_df: {fmr_duplicates.shape[0]}\")\n",
    "\n",
    "# ðŸ‘€ Display the duplicated rows\n",
    "print(fmr_duplicates.sort_values(by=['FIPS', 'State', 'County']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af6b01-d58a-4bfd-8b62-ccc26ce9898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Find rows with duplicated FIPS only\n",
    "fmr_dup_fips = fmr_df[fmr_df.duplicated(subset='FIPS', keep=False)]\n",
    "\n",
    "# ðŸ“Š Show how many FIPS codes are duplicated\n",
    "print(f\"â— Rows with duplicated FIPS in fmr_df: {fmr_dup_fips.shape[0]}\")\n",
    "\n",
    "# ðŸ‘€ Display the duplicated rows sorted by FIPS\n",
    "print(fmr_dup_fips.sort_values(by='FIPS'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741c5c3-9656-46bb-83b6-07af2b59a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create FIPS lookup from merged_df_1 and rename FIPS\n",
    "fips_lookup_df = merged_df_1[['State', 'County', 'FIPS']].drop_duplicates()\n",
    "fips_lookup_df = fips_lookup_df.rename(columns={'FIPS': 'FIPS_corrected'})\n",
    "\n",
    "# Step 2: Merge into fmr_df using State + County\n",
    "fmr_df = pd.merge(\n",
    "    fmr_df,\n",
    "    fips_lookup_df,\n",
    "    on=['State', 'County'],\n",
    "    how='left'  # Keep all rows from fmr_df\n",
    ")\n",
    "\n",
    "# Step 3: Show unmatched rows (where merge failed = no FIPS_corrected found)\n",
    "mismatched_names = fmr_df[fmr_df['FIPS_corrected'].isna()]\n",
    "\n",
    "print(f\"â— Rows with unmatched State + County: {mismatched_names.shape[0]}\")\n",
    "print(mismatched_names[['State', 'County', 'FIPS']])\n",
    "\n",
    "# Optional: Show rows where FIPS exists but differs from corrected one\n",
    "fips_conflicts = fmr_df[\n",
    "    (fmr_df['FIPS_corrected'].notna()) &\n",
    "    (fmr_df['FIPS'] != fmr_df['FIPS_corrected'])\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ” Rows with differing FIPS values: {fips_conflicts.shape[0]}\")\n",
    "print(fips_conflicts[['State', 'County', 'FIPS', 'FIPS_corrected']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02524a1b-1398-4b75-a1fa-07eeb1056983",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b126e5-218d-4b92-97f3-c661cb9f479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Manually confirmed FIPS values\n",
    "manual_fips_fixes_2 = {\n",
    "    ('IN', 'DeKalb County'): '18033',\n",
    "    ('IN', 'LaGrange County'): '18087',\n",
    "    ('IN', 'LaPorte County'): '18091',\n",
    "    ('AK', 'Anchorage Municipality'): '02020',\n",
    "    ('AK', 'Juneau City and Borough'): '02110',\n",
    "    ('AK', 'Petersburg Borough'): '02195',\n",
    "    ('AK', 'Sitka City and Borough'): '02220',\n",
    "    ('AK', 'Wrangell City and Borough'): '02275',\n",
    "    ('AK', 'Yakutat City and Borough'): '02282',\n",
    "    ('MO', 'Sullivan part'): '29211',  # Closest FIPS: Sullivan County, MO\n",
    "    ('NM', 'De Baca County'): '35011',\n",
    "    ('PA', 'McKean County'): '42083'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa0111-701f-4efa-b15c-8da347ae56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Update FIPS_corrected using manual fixes only where FIPS_corrected is missing\n",
    "fmr_df['FIPS_corrected'] = fmr_df.apply(\n",
    "    lambda row: manual_fips_fixes.get((row['State'], row['County']), row['FIPS_corrected']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc3e72-c7a9-48f4-a127-50968dff5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c822345-ed40-4ce0-85fc-5751745864bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns\n",
    "print(fmr_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cf6e4-27a6-41ee-92d1-743fe894aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop all other FIPS_corrected columns except the last one\n",
    "fmr_df = fmr_df.drop(columns=[col for col in fmr_df.columns if col.startswith('FIPS_corrected_')])\n",
    "\n",
    "# Step 2: Rename the surviving one to keep it clear\n",
    "fmr_df = fmr_df.rename(columns={'FIPS_corrected': 'FIPS_corrected_clean'})\n",
    "\n",
    "# Step 3 (optional): Create final FIPS column â€” fallback to original if corrected is missing\n",
    "fmr_df['FIPS_final'] = fmr_df['FIPS_corrected_clean'].combine_first(fmr_df['FIPS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a37db-ec15-4d8d-865c-83f3edf7f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f30d7-c054-400a-a246-49c772d497c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop both FIPS and corrected version (after FIPS_final has been created)\n",
    "fmr_df = fmr_df.drop(columns=['FIPS', 'FIPS_corrected_clean'])\n",
    "\n",
    "# âœ… Check result\n",
    "print(fmr_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc6fda-5711-4af0-9426-40d1a2183340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop original FIPS columns\n",
    "fmr_df = fmr_df.drop(columns=['FIPS', 'FIPS_corrected_clean'], errors='ignore')\n",
    "\n",
    "# Step 2: Rename final column\n",
    "fmr_df = fmr_df.rename(columns={'FIPS_final': 'FIPS'})\n",
    "\n",
    "# Step 3: Reorder â€” move FIPS to 3rd column\n",
    "cols = fmr_df.columns.tolist()\n",
    "fips_index = cols.index('FIPS')\n",
    "# Remove FIPS and insert at index 2 (3rd position)\n",
    "cols.insert(2, cols.pop(fips_index))\n",
    "fmr_df = fmr_df[cols]\n",
    "\n",
    "# âœ… Preview\n",
    "print(fmr_df.columns[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf1845-8b4c-4088-b3d9-c8b2a51f9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54b16e-6af4-4c46-ad35-3670ad7f1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check full row duplicates\n",
    "full_duplicates = fmr_df.duplicated()\n",
    "print(f\"â— Full duplicate rows: {full_duplicates.sum()}\")\n",
    "\n",
    "# Check duplicates based on FIPS, State, and County\n",
    "key_duplicates = fmr_df.duplicated(subset=['FIPS', 'State', 'County'], keep=False)\n",
    "print(f\"â— Duplicate rows based on FIPS + State + County: {key_duplicates.sum()}\")\n",
    "\n",
    "# Optional: Show those rows\n",
    "if key_duplicates.sum() > 0:\n",
    "    print(fmr_df[key_duplicates].sort_values(by=['FIPS', 'State', 'County']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac535b1-dc49-4064-9f94-40bfdc3e33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count full duplicates\n",
    "full_dups = fmr_df.duplicated()\n",
    "print(f\"â— Fully duplicated rows to drop: {full_dups.sum()}\")\n",
    "\n",
    "# Step 2: Drop them\n",
    "fmr_df = fmr_df.drop_duplicates()\n",
    "\n",
    "# âœ… Confirm new shape\n",
    "print(f\"âœ… fmr_df shape after dropping full duplicates: {fmr_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed9a74-7ef8-4e66-b814-0cef71055946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on FIPS, State, and County\n",
    "key_duplicates = fmr_df.duplicated(subset=['FIPS', 'State', 'County'], keep=False)\n",
    "\n",
    "# Filter and show\n",
    "duplicate_rows = fmr_df[key_duplicates].sort_values(by=['FIPS', 'State', 'County'])\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cf254-3f1e-4d9c-8f42-25dc492d5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by FIPS, State, County and average all other columns\n",
    "fmr_df = fmr_df.groupby(['FIPS', 'State', 'County'], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "# âœ… Confirm shape after deduplication\n",
    "print(f\"âœ… Shape after resolving duplicates: {fmr_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa0d15-b0d4-4af5-a569-5036cd2b1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Check for fully duplicated rows\n",
    "full_dups = fmr_df.duplicated()\n",
    "print(f\"â— Full duplicate rows: {full_dups.sum()}\")\n",
    "\n",
    "# ðŸ” Check for duplicates based on FIPS, State, County\n",
    "key_dups = fmr_df.duplicated(subset=['FIPS', 'State', 'County'], keep=False)\n",
    "print(f\"â— Duplicate rows based on FIPS + State + County: {key_dups.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b71739-9ef5-4500-a8d8-e9e8b2aebf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"merged_df_1 rows: {merged_df_1.shape[0]}\")\n",
    "print(f\"merged_df_2 rows: {merged_df_2.shape[0]}\")\n",
    "print(f\"fmr_df rows: {fmr_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e379f-e97f-428e-9ce3-d56a07f8b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§± Step 1: Start with merged_df_1 as the base\n",
    "# We'll merge everything into this DataFrame\n",
    "\n",
    "# ðŸ“¦ Step 2: Merge population data (merged_df_2) using FIPS, State, County\n",
    "df_merged = pd.merge(\n",
    "    merged_df_1,\n",
    "    merged_df_2,\n",
    "    on=['FIPS', 'State', 'County'],\n",
    "    how='left',  # Keep all rows from merged_df_1, even if there's no match\n",
    "    suffixes=('', '_pop')\n",
    ")\n",
    "\n",
    "# ðŸ  Step 3: Merge FMR data (fmr_df) using FIPS, State, County\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    fmr_df,\n",
    "    on=['FIPS', 'State', 'County'],\n",
    "    how='left',  # Again, keep all rows from base dataset\n",
    "    suffixes=('', '_fmr')\n",
    ")\n",
    "\n",
    "# âœ… Step 4: Confirm shape and preview\n",
    "print(f\"âœ… Final merged dataset shape: {df_merged.shape}\")\n",
    "print(df_merged[['FIPS', 'State', 'County']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746aa5e-acf9-4f84-b9d0-c7bda8d7b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check missing values\n",
    "missing_summary = df_merged.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "# Step 2: Identify unmatched rows from merged_df_2 and fmr_df\n",
    "merged_2_columns = [col for col in merged_df_2.columns if col not in ['FIPS', 'State', 'County']]\n",
    "fmr_columns = [col for col in fmr_df.columns if col not in ['FIPS', 'State', 'County']]\n",
    "\n",
    "missing_merged_2 = df_merged[merged_2_columns].isna().all(axis=1).sum()\n",
    "missing_fmr = df_merged[fmr_columns].isna().all(axis=1).sum()\n",
    "\n",
    "# Step 3: Check for key-based duplicates\n",
    "dup_count = df_merged.duplicated(subset=['FIPS', 'State', 'County']).sum()\n",
    "\n",
    "# Step 4: Print summary\n",
    "summary = {\n",
    "    \"Total rows\": df_merged.shape[0],\n",
    "    \"Rows missing any value\": int(df_merged.isna().any(axis=1).sum()),\n",
    "    \"Rows missing all merged_df_2 columns\": int(missing_merged_2),\n",
    "    \"Rows missing all fmr_df columns\": int(missing_fmr),\n",
    "    \"Duplicate rows (FIPS + State + County)\": int(dup_count),\n",
    "    \"Columns with missing values\": missing_summary.to_dict()\n",
    "}\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3bdd4-db92-48e7-a670-b64e26fab4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = df_merged[df_merged.isna().any(axis=1)]\n",
    "missing_rows.to_excel(\"missing_values_review.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63ddb4-73a2-4adc-a93b-2ab3ebbd00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['missing_fmr'] = df_merged[fmr_columns].isna().all(axis=1)\n",
    "df_merged['missing_demo'] = df_merged[merged_2_columns].isna().all(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470f742-b083-41e8-aff4-a73157171498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your export folder path\n",
    "export_folder = \"/Users/emilsafarov/Library/CloudStorage/OneDrive-Personal/CF/A6/1_sourceing_open_data/US project/Data/Output data/untitled folder\"\n",
    "\n",
    "# Export as Excel (.xlsx)\n",
    "df_merged.to_excel(f\"{export_folder}/final_merged_dataset.xlsx\", index=False)\n",
    "\n",
    "# Export as CSV\n",
    "df_merged.to_csv(f\"{export_folder}/final_merged_dataset.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Exported both Excel and CSV to the output folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
